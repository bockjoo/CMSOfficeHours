<style type="text/css" media="all">
pre {text-align: left; padding: 10px; color: black; font-size: 12px;}
pre.cmd {background-color: lightgrey;}
pre.config {background-color: lightblue;}
pre.code {background-color: lightpink;}
pre.output {background-color: lightgreen;}
pre.file {background-color: lightyellow;}
pre.note {background-color: white;}
</style>

---+ The !CMS Newcomer Handbook
---

This is a collection of useful programming tips, physics knowledge, and references to help you survive in the !CMS environment.<br />
Use the *Table of Contents* below or just *use your browser's built-in search function* to look for your keyword of interest.
---

<!-- %TOC{title="Goodies:"}% -->
 %TOC{}%
---
---++ Text background color convention

In this tutorial page, the following text background color convention is used:
<verbatim class="cmd">GREY: For commands.</verbatim>
<verbatim class="output">GREEN: For the output of executed commands.</verbatim>
<verbatim class="code">PINK: For CMSSW parameter-set configuration files.</verbatim>
<verbatim class="file">YELLOW: For any other type of file.</verbatim>

---

*A couple of notes on using this Handbook:*

Text in angled brackets should be replaced:
<verbatim class="cmd">mkdir /home/<username>/public_html/ </verbatim>

Becomes (in my case):
<verbatim class="cmd">mkdir /home/rosedj1/public_html/ </verbatim>

---
---++ Acronyms and Descriptions

| *Acronym* | *Stands For...* | *Short Description* |
| AOD | Analysis Object Data | |
| API | Application Programming Interface | |
| ARC | Analysis Review Committee | |
| ASIC | Application-Specific Integrated Circuit | |
| ASO | Asynchronous Stage Out | |
| BPH | B-physics and general quarkonia | |
| BR | Background Region | |
| BRIL | Beam Radiation Instrumentation and Luminosity | |
| B2G | Beyond 2 Generations | |
| CADI | CMS Analysis Database Interface | Database for CMS papers, Analysis Notes, etc. |
| CAF | CERN Analysis Facility | |
| CB | Crystal Ball | Gaussian pdf convoluted with a Power Law pdf |
| CINCO | CMS Information on Conferences | The main hub for all CMS conference info |
| CMSSW | CMS SoftWare | Large collection of C++, Python, ROOT libraries for coding |
| CR | Control Region | The region _around_ your signal region. %BR% If your Data/MC is good in your CR, then it should be good in your blinded SR |
| CRAB | CMS Remote Analysis Builder | Huge parallel-processing system for submitting large jobs on computers across the world! |
| CSS | Cascading Style Sheets | |
| CSV | Comma-Separated Values | Text file that uses a comma to separate one value from another |
| CSV | Combined Secondary Vertex | |
| CVMFS | Cern Virtual Machine File System | |
| CWR | Collaboration-Wide Review | |
| DAS | Data Aggregation Service (used to be DBS, Data Bookkeeping Service) | |
| DB | Data Base | |
| DPF | Division of Particles and Fields | |
| DQM | Data Quality Monitoring | |
| EDM | Event Data Model | |
| EOS | | |
| EVE | Event Visualization Environment | |
| EWPT | ElectroWeak Precision Tests | |
| EWSB | ElectroWeak Symmetry Breaking | |
| FC | Flavor, Charge | |
| FOUT | Fraction Out (outside fiducial cuts, for example) | |
| FSR | Final-State Radiation | |
| FTR | | |
| FWL | Frame Work Lite | |
| GEM | Gas Electron Multiplier | |
| GFAL | Grid File Access Library | |
| Gsf | Gaussian-sum filter | |
| GT | Global Tag | |
| HEFT | Higgs Effective Field Theory | |
| HELAS | HELicity Amplitude Subroutine library | |
| HEP | High Energy Physics | |
| HF | Heavy Flavor | |
| HI | Heavy Ion | |
| JEC | Jet-Energy Corrections | |
| JER | Jet-Energy Resolution | |
| JP | Jet Probability | |
| JSON | JavaScript Object Notation | |
| KF | Kalman Filter | |
| LCG | LHC Computing Grid | |
| LFN | Logical File Name | |
| LFV | Lepton Flavor Violation | |
| LHE | Les Houches Events | |
| LPC | LHC Physics Center | |
| LPC | LHC Physics Center at CERN | |
| LSF | Load Sharing Facility | |
| LXPLUS | Linux Public Login User Service | |
| MCFM | Monte Carlo for FeMtobarn processes | |
| MCM | Monte Carlo Manager | |
| ME | MadEvent | |
| MELA | Matrix Element Likelihood Approach (or Analysis) | |
| MG | MadGraph | |
| MGM | | |
| MLM | (something to do with jet multiplicity) | |
| MIP | Minimum Ionizing Particle | |
| MSSM | Minimal Supersymmetric Standard Model | |
| MTD | MIP Timing Detector | |
| NNLL | Next-to-Next-to-Leading-Logarithmic | |
| OSSF | Opposite Sign, Same Flavor | |
| PAG | Physics Analysis Groups | |
| PAS | Physics Analysis Summary | |
| PAT | Physics Analysis Toolkit | |
| PAW | Physics Analysis Workstation | |
| PDF | Parton Distribution Function | |
| PDF | Particle Data Group | |
| PD | Primary Dataset | |
| PF | Particle Flow | |
| PFN | Physical File Name | |
| PhEDEx | Physics Experiment Data Export service | |
| POG | Physics Objects Groups | |
| PPD | Physics Performance Dataset | |
| PPS | Precision Proton Spectrometer | |
| PROOF | Parallel ROOT Facility | |
| PS | Parton Shower | |
| PUPPI | Pileup Per Particle Identification | |
| PV | Primary Vertex | |
| PW | Password | The thing which, along with all your associated accounts, birthdates, %BR% and credit card information, you should email to the author. |
| PWG | Physics Working Group | |
| RAID | Redundant Array of Independent Disks | |
| RECO | Reconstructed | |
| RelVal | new Release Validation | |
| ROC | ReadOut Chip | |
| ROC | Receiver Operating Characteristic | |
| RSS | Resident Set Size | |
| SCRAM | Software Computing, Release and Management | |
| SCRAM | Source Configuration, Release and Management | |
| SIP | Significance of the 3D Impact Parameter (e.g. &lt; 4) | |
| SLC | Scientific Linux CERN | |
| SB | Signal, Background | |
| SF | Scale Factor | |
| SR | Signal Region | |
| SUSY | SuperSymmetry | |
| SV | Secondary Vertex | |
| T2 | Tier 2 server | |
| UE | Underlying Event | |
| UFO | Unidentified Falling Object --or-- Universal FeynRules Output | |
| UN | Username | |
| VO | Virtual Organization | |
| VOMS | Virtual Organization Membership Service | |
| WLCG | | |
| WN | Worker Node | |
| 2HDM | Two-Higgs-Doublet Model | |
| 2P2F | 2 Pass 2 Fail (2 leptons pass tight selection, 2 fail) | |
| 3P1F | 3 Pass 1 Fail (3 leptons pass tight selection, 1 fails)  | |
| 3P1F | 3 Pass 1 Fail | In reference to final-state particles (e.g. 3 leptons pass tight selection, but 1 fails) |
---

---++ *Frequent Abbreviations*

dir = directory
cmd = command

---++ Programming Languages:

---+++ ROOT/PyRoot

Make sure that you are working in a CMSSW area and have already done =cmsenv=.

*ROOT* <br />
Allows you to play with and process NTuples and other .root files.<br />
Also, it is an interpreter for C++!

PyROOT is just using Python while importing ROOT's libraries.<br />
You can accomplish this by doing:

An "NTuple" is a .root file which has at least one TTree inside of it.
These TTrees usually have lots of "branches" in them.
Each branch could be a lot of things, like: a kinematical variable, RECO objects, etc.
Think of a branch as a line of buckets, where each each bucket is an event:
- the first bucket contains information about the first event, second bucket -&gt; second event, ...
Branches can contain other branches...
Eventually you will get to a leaf. A leaf is usually a histogram with interesting information.

Check which version of ROOT you have:
echo $ROOTSYS
Change version:
source /cvmfs/cms.cern.ch/slc6_amd64_gcc630/lcg/root/<version>

Can do:
`root --help' to see options
-l   &lt;== do not show splash screen
-b    &lt;== run in batch mode without graphics
-q   &lt;== quit after processing command line macro files

Load a root file into ROOT and open the TBrowser:
root -l slimMiniAOD_MC_MuEle.root;
or
TFile *theFile = TFile::Open("root://cmseos.fnal.gov//store/user/cmsdas/2018/pre_exercises/CMSDataAnaSch_MiniAODZMM730pre1.root");
or
TFile f("&lt;my_file.root&gt;")
or TFile.Open("<myfile>")

See what's inside the root file:
.ls     or
.ls()   or
f-&gt;ls()

Other ways to point to files:
'root://cmseos.fnal.gov//store/user/cmsdas/2018/pre_exercises/fourth_set/slimMiniAOD_data_MuEle_1.root', (at USCMS)
'file:/afs/cern.ch/work/d/dmoon/public/CMSDAS_Files/Exe4/slimMiniAOD_data_MuEle_1.root', (at lxplus or Bari)
'file:/cmsdas/data/pre_exercises/Exe4/slimMiniAOD_data_MuEle_1.root', (at KNU)
'file:/pnfs/desy.de/cms/tier2/store/user/your_username/DoubleMuon/crab_CMSDAS_Data_analysis_test0/160718_090558/0000/slimMiniAOD_data_MuEle_1.root' (at nafhh-cms)

Can convert ROOT stuff into python stuff —&gt; PyRoot
SUPER HELPFUL
Brendan Regnery taught me about:
branch_in_array = root_numpy.root2array(“&lt;path_to_root_file&gt;, &lt;tree_name&gt;, &lt;branch_name&gt;”)

Example rootlogon.C:
gSystem-&gt;Load("libFWCoreFWLite.so");
FWLiteEnabler::enable();
gSystem-&gt;Load("libDataFormatsFWLite.so");
gROOT-&gt;SetStyle ("Plain");
gStyle-&gt;SetOptStat(111111);

ROOT can explore the contents of files just like a file system:
gDirectory.pwd()
gDirectory.ls()

Open the TBrowser:
TBrowser b;

TTrees:
Make a tree pointer:
root -l /raid/raid7/lucien/Higgs/DarkZ-NTuple/20180706/ZD_UpTo0j_MZD20_Eps1e-2_klo.root   
then:
TTree* t  = (TTree*)_file0-&gt;Get("Ana/passedEvents")

Go inside a file and access the TTree:
f-&gt;Get(“&lt;dir/tree&gt;”)
- for example: file1-&gt;Get("Ana/passedEvents")

If your histogram is called a TObject instead of TH1F (or similar), then do this instead:
TH1F * h = (TH1F*)gDirectory-&gt;GetList()-&gt;FindObject("<hist>")

Generate a skeleton of your analysis in files "name.C" and "name.h":
TTree myTree
myTree.MakeClass("name", "options")

Useful TTree methods:
(NOTE: C++ pointers use the =-&gt;= command.)
<verbatim class="cmd">
t->Print()       	   <== see what branches your tree has
t->Show(0)                 <== shows the zeroth event
t->Scan()                  <== scans the first 25 events
t->Scan("<branch_name>")
t->Scan("GENlep_id[]")
t->Scan("nPV:nJet")	<=== scan multiple events along nPV branch through the nJet branch
t->GetEntries() 		<=== gives total number of events in N-Tuple
t->GetEntries("passedFullSelection==1")
t->GetEntries("passedFiducialSelection==1")
t->GetEntries("Sum$(abs(GENlep_id[])==11)==4")
t->GetEntries("Sum$(abs(GENlep_id[])==11)>=4")
t->GetEntries("Sum$(abs(GENlep_id[])==11)>=4 && passedFiducialSelection==1")
t->GetEntry(2)			<== puts you at entry2 and allows you to extract branch info
Then: t->massZ		<== Suppose massZ is a branch, print out massZ value of entry2
t->SetBranchAddress(?<branch>?, &pointer)
t->GetEvent(<int>)
t->Draw(?pt3?)						<=== pt3 is a branch of tree t (TTrees can draw branches)
t->Draw("<branch>","<cuts>")
t->Draw("ebeam","px>10 && zv<20")
t->Draw("ebeam","(1/e)*(sqrt(z)>3.2)")		<== apply a weight of 1/e to all events whose sqrt(z)>3.2
t->Draw("patMuons_slimmedMuons__PAT.obj.eta()","abs(patMuons_slimmedMuons__PAT.obj.eta())<1.2","")

passedEvents->Draw("mass4l", "met>50", "")	# Third option is drawing option
passedEvents->Draw("met:mass4l", "", "colz")	# MET vs. mass4l

Make 1-D, 2-D projection of tree into histogram:
t->Project("<histoname>", "met:mass4l")

One way to make a histogram using the data stored in a TTree:
Go look up the '>>' trick. Maybe it's already here in the notes...

It may be very powerful and useful to use TSelectors:
t->MakeSelector("MySelector")		<== generates a skeleton macro 
t->Process(selector)				<== runs the Process section of selector

If the object is not a pointer, then you have to use the `.` operator instead of `->`:
t.Show()</verbatim>

passedEvents is a TTree! So a TTree can draw histograms.

Combine all ROOT files into one (hadd = "histogram add"):
hadd &lt;new_root_file_name&gt;.root &lt;root_files_to_be_combined&gt;.root
e.g.,
hadd -f ZD_UpTo0j_MZD25_Eps1e-2_klo.root ZD_UpTo0j_MZD25_Eps1e-2_klo_*.root
hadd mergedfile.root file1.root file2.root ... fileN.root
- the -f flag forces the new file to be produced, even if the file already exists

I forked Lucien's hadding repo:
https://github.com/rosedj1/UFHZZ4l-T2Hadd

Histograms
***It's probably more efficient to set the binwidth than the num_bins!

There is an "overflow bin" on the right edge of the histogram that collects entries which lie outside the
histogram x range. These entries are NOT counted in the statistics (mean, stdev), BUT they are counted
as new entries!
- There's also an underflow bin
Therefore, entries in overflow bins DO count towards total entries, but not towards statistics, like Integral()

Rebinning should be rather easy!
- there is a Rebin method
- Errors are automatically recalculated
Normalizing Histos:
Use: Scale(1/h-&gt;Integral)

TH1F * h = new TH1F ("h","My Histogram",100,-20,20)
h-&gt;FillRandom("gaus", 5000)      &lt;== Fill h1 with 5000 random points pulled from Gaussian Distribution
h-&gt;Fill(gRandom-&gt;Gaus(4,2))   &lt;== Fill h1 with a single point pulled from Gaussian with mu=4, sigma=2
                        (do a for loop to fill it with many points)
   for (int i=0; i&lt;1000; i++) {h-&gt;Fill(gRandom-&gt;Gaus(40,15));}
h-&gt;GetEntries()            &lt;== Returns how many total values have been put into the bins
h-&gt;GetMaximum()            &lt;== Returns the number of entries inside the bin which holds the most entries
h-&gt;GetMinimum()            &lt;== Returns the number of entries inside the bin which holds the fewest entries
h-&gt;GetBinContent(<int bin_num>)   &lt;== Returns the number of entries inside bin number bin_num
h-&gt;GetMaximumBin()         &lt;== Tells you which bin holds the most entries; Returns the bin number(not x value of bin!)

   mumuMass-&gt;GetXaxis()-&gt;GetBinCenter(mumuMass-&gt;GetMaximumBin())      &lt;== returns most-probable value of histo

h-&gt;GetMaximumStored()            &lt;== ???
h-&gt;GetMean()                     &lt;== Get average of histogram
h-&gt;GetStdDev()                  &lt;== Get standard deviation of histo
h-&gt;GetXaxis()-&gt;GetBinCenter(<int bin>)   &lt;== returns the x value where the center of bin is located
h-&gt;GetNbinsX()                  &lt;== Returns the number of bins along x axis
h-&gt;Fill(<int bin_num>, <double val>)      &lt;== Fills bin number bin_num with value val
h-&gt;SetBinContent(<int bin>, <double val>)   &lt;== Deletes whatever is in bin number bin, and fills it with value val
                                 (this counts as adding a NEW entry!)
h-&gt;SetAxisRange(double <xmin>, double <xmax>, "<X or Y>")    &lt;==
h-&gt;IntegralAndError(,,<err>)               &lt;== calculates the integral
    - err will store the error that gets calculated
    - so before you execute the IntegralAndError, first do err = Double(2) to create the err variable
h-&gt;SetLogy()                     &lt;== set y axis to be log scale

TH2F
h2-&gt;Integral()                           &lt;== calculate integral over ALL bins
h2-&gt;IntegralAndError(xbin1, xbin2, ybin1, ybin2, err)   &lt;== calculates the integral over square region, specified by bins
- err will store the error that gets calculated
- so before you execute the IntegralAndError, first do err = Double(2) to create the err variable
h2-&gt;Draw("COLZ1")   &lt;== "COL" means color, "Z" means draw the color bar, "1" makes all cells&lt;=0 white!

Bin convention:
bin = 0; underflow bin
bin = 1; first bin with low-edge xlow INCLUDED
bin = nbins; last bin with upper-edge xup EXCLUDED
bin = nbins+1; overflow bin

h-&gt;Draw()
h-&gt;Draw("HIST")   &lt;== Make sure to draw a histogram
h-&gt;Draw("HIST e")   &lt;== Draw histogram with error bars ( where err = sqrt(num_entries_in_bin) )

Canvases:
Make a pointer:
TCanvas * c = new TCanvas()
Make an object (from TCanvas class):
c = TCanvas("&lt;internal_name&gt;", "&lt;canvas_title&gt;", int &lt;num_x_pixels&gt;, int &lt;num_y_pixels&gt;)

TGraph:
tg = TGraph(<int num_of_points>, &lt;x_array&gt;, &lt;y_array&gt;)
Slightly tricky when setting axis label for TGraphs:
tg.GetXaxis().SetTitle("&lt;x_title&gt;")
tg.Draw("APC")
Drawing Options   Description
"A"   Axis are drawn around the graph
"I"   Combine with option 'A' it draws invisible axis
"L"   A simple polyline is drawn
"F"   A fill area is drawn ('CF' draw a smoothed fill area)
"C"   A smooth Curve is drawn
"*"   A Star is plotted at each point
"P"   The current marker is plotted at each point
"B"   A Bar chart is drawn
"1"   When a graph is drawn as a bar chart, this option makes the bars start from the bottom of the pad. By default they start at 0.
"X+"   The X-axis is drawn on the top side of the plot.
"Y+"   The Y-axis is drawn on the right side of the plot.
"PFC"   Palette Fill Color: graph's fill color is taken in the current palette.
"PLC"   Palette Line Color: graph's line color is taken in the current palette.
"PMC"   Palette Marker Color: graph's marker color is taken in the current palette.
"RX"   Reverse the X axis.
"RY"   Reverse the Y axis.

Multigraph:
mg = TMultiGraph ("&lt;internal_name&gt;", "")

mg.SetMaximum(<maxval>)      &lt;== set y-axis to <maxval>

You can fit functions to the histogram:
mumuMass-&gt;Fit("gaus")
- or use a histogram
g1 = new TF1("m1","gaus",85,95);
mumuMass-&gt;Fit(g1,"R");
- here mumuMass is the name of the histogram

To learn more about fitting functions, go to LPC machines at Fermilab:
/uscms/home/drosenzw/nobackup/YOURWORKINGAREA/CMSSW_9_3_2/src/
And follow these directions:
https://twiki.cern.ch/twiki/bin/viewauth/CMS/SWGuideCMSDataAnalysisSchoolPreExerciseFourthSet

TTreeViewer:
TTreeViewer TV(&lt;Tree_Name&gt;);
   or
tree-&gt;StartViewer()

TTreeReader:
- Haven't explored this yet, but Bhargav says it's easy to open up TTrees

Class to play with 4-vectors:
https://root.cern.ch/doc/master/classTLorentzVector.html#a16b997917278ac1538b4a4ee3661fa23

USING LATEX
{
   TCanvas c1("c1");
   c1.SetBottomMargin(0.15);
   TH2F h2("h2","h2 with latex titles",40,0,40,20,0,10);
   h2.GetXaxis()-&gt;SetTitleOffset(1.4);
   h2.GetXaxis()-&gt;SetTitle("#left| #frac{1}{1 - #Delta#alpha}
#right|^{2} (1+cos^{2}#theta)");
   h2.GetYaxis()-&gt;SetTitle("#frac{2s}{#pi#alpha^{2}}
#frac{d#sigma}{dcos#theta}");
   h2.Draw();
}

Legends:
leg = TLegend(xmin, ymin, xmax, ymax)    &lt;== (all floats between 0 and 1, as a proportion of the x or y dimension)
leg.AddEntry()
You can

If you get a GLIB seed error, just do:
cmsenv

How to "pretty up" your plot:
h.GetXaxis().SetTitle("massZ(GeV)")   &lt;== put title on X axis
h.GetYaxis().SetTitleOffset(1.3)      &lt;== move Y axis up or down a bit

—————————————
RooFit
Philosophy: Each math symbol is a separate C++ object
x, a variable         &lt;==&gt; RooRealVar
f(x), a function      &lt;==&gt; RooAbsReal
F, a pdf            &lt;==&gt; RooAbsPdf
vector (space point)   &lt;==&gt; RooArgSet
integral            &lt;==&gt; RooRealIntegral
list of space points      &lt;==&gt; RooAbsData
formula with variables   &lt;==&gt; RooFormulaVar
- btw, "Abs" stands for "abstract"

How some of these work:
massZ = RooRealVar ("name", "title", value, min, max)
RooArgSet (w.var.("massZErr"))
RooArgList (lambda_,  massZErr)
RooFormulaVar ("sigma","@1*@0", RooArgList (lambda_,  massZErr))

Different Built-In PDFs:
bw = RooBreitWigner ("<name>", "", <var>, <mean>, <gamma>)
cb = RooCBShape ("<name>", "", <var>, <mean>, <sigma>, <alpha>, <n>)
ex = RooExponential ("name", "title", <var>, tau)

You can convolute PDFs:
RooFFTConvPdf ("<name>", "", variable, pdf1, pdf2)

Make a workspace:
w = ROOT.RooWorkspace ("w")
w.Print()               &lt;== see what's inside your workspace
w.pdf(<RooAddPdf>).fitTo(<RooDataSet>)
w.var("<name>").getVal()   &lt;==

Make a RooDataSet:
mydataset = RooDataSet (name,title,data, self.Data_Zlls.get(), "1", "weight")
mydataset.Print()      &lt;== get basic info about dataset
mydataset.numEntries()   &lt;== get entries of the dataset
mydataset.ls()         &lt;== get a little info about dataset
mydataset.get()      &lt;== returns the coordinates of the current RooArgSet (vectors?)
mydataset.GetName()   &lt;== returns name
mydataset.GetTitle()   &lt;== returns title
mydataset.add(<val>)   &lt;== add some value to your dataset?   

Convolute PDFs into a RooAbsPdf object (model):
CBxBW     = RooFFTConvPdf ("CBxBW","CBxBW", massZ, BW, CB)
bkg = RooExponential ("bkg","bkg", massZ, tau)
fsig = RooRealVar ("fsig","signal fraction", self.shapePara["fsig"])
model = RooAddPdf ("model","model", CBxBW, bkg, fsig)

---+++ Python

---+++ Bash

---+++ C++

---++ CMSSW (CMS SoftWare):

https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookConfigFileIntro<br /><br />cmsrel CMSSW_X_Y_Z<br />- install the CMSSW environment in a new dir, version X_Y_Z, like e.g. 9_4_2<br />   - 8_0_X = 2016 data<br />   - 9_4_X = 2017 data<br />   - 10_2_X = 2018 data<br />- once inside, be sure to do: cmsenv to "load" the environment variables (sets up your runtime environment)!<br />- You will have to use different CMSSW versions for different years' data!<br /><br />See what versions of CMSSW are available:<br />scram list -a<br />scram list -a | egrep "CMSSW_9_4_X" &gt; cmssw.txt<br /><br />***POTENTIALLY MERGE THE STUFF BELOW INTO CRAB SECTION***<br />Config Files:<br />Two kinds of config files:<br />1. config files for submitting CRAB jobs (crabConfig)<br />- You need configuration files to tell how CRAB how to deal with the jobs you want processed.<br />2. config files to generate MC events (MC_config)<br /><br />Example to generate an MC config file:<br />cmsDriver.py MinBias _13TeV_pythia8_TuneCUETP8M1_cfi --conditions auto:run2_mc -n 10 --era Run2_2016 --eventcontent FEVTDEBUG --relval 100000,300 -s GEN,SIM --datatier GEN-SIM --beamspot Realistic50ns13TeVCollision --fileout file:step1.root --no_exec --python_filename CMSDAS_MC_generation_cfg.py<br />- newly produced config file is called CMSDAS_MC_generation_cfg.py<br /><br />Use cmsRun to load modules stored in a configuration file:<br />cmsRun CMSDAS_MC_generation_cfg.py<br /><br />You can just make sure everything properly compiles by doing:<br />python CMSDAS_MC_generation_cfg.py<br />- if it returns no errors, you should be good to go!<br />- Do this before submitting CRAB jobs<br /><br />A config file allows you to set all the parameters you want for a job.<br />- They usually start with this line:<br />import FWCore.ParameterSet.Config as cms      &lt;== imports our CMS-specific Python classes and functions<br />- And have these as the guts:<br />   - A source (which might read Events from a file or create new empty events) <br />   - A collection of modules (e.g. EDAnalyzer, EDProducer, EDFilter) which you wish to run<br />   - An output module to create a ROOT file which stores all the event data<br />   - A path which will list in order the modules to be run <br /><br />A configuration file written using the Python language can be created as: <br />- a top level file, which is a full process definition (naming convention is _cfg.py ) which might import other configuration files <br />- external Python file fragment, which are of two types:<br />   - those used for module initialization (naming convention is _cfi.py)      &lt;== configuration fragment include<br />   - those used as configuration fragment (naming convention is _cff.py) &lt;== configuration fragment file?<br /><br />process.load()      &lt;== Import fragment to top level, also attaches imported objects<br /><br />Standard fragments are available in the CMSSW release's Configuration/StandardSequences/python/ area. They can be read in using syntax like <br />process.load("Configuration.StandardSequences.Geometry_cff")<br /><br />The word "module" has two meanings. A Python module is a file containing Python code and the word also refers to the object created by importing a Python file. In the other meaning, EDProducers, EDFilters, EDAnalyzers, and OutputModules are called modules. <br /><br /><br />Use PhEDEx to transfer datasets between storage areas:<br />https://cmsweb.cern.ch/phedex/prod/Request::Create?type=xfer#<br /><br />ED Analyzer <span>= .cc file<br />python config file<br /><br />git cms-merge-topic &lt;github_user&gt;:&lt;branch_of_any_repo&gt;<br />- I think this merges the branch and its contents into local directory?<br /><br /></span>

---+++ xrootd

https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookXrootdService<br />xrootd is a command to retrieve Any data, Anytime, Anywhere (AAA) on the CMS servers from anywhere in the world. <br />Uses logical file names (LFN) to find files <br /><br />For the US, use:<br />cmsxrootd.fnal.gov<br /><br />Must first do:<br />voms-proxy-init -voms cms<br /><br />e.g.<br />TFile f = TFile::Open("root://cmsxrootd.fnal.gov///store/mc/SAM/GenericTTbar/GE.root");<br /><br />If you wish to check if your desired file is actually available through AAA, execute the command:<br />`xrdfs cms-xrd-global.cern.ch locate /store/path/to/file’<br />(xrd = xrootd, fs = file search?)<br /><br />In a MC cfg.py file, use: <br />fileNames = cms.untracked.vstring('root://cmsxrootd.fnal.gov//store/myfile.root')<br /><br />Change password<br />In command line, do:<br />yppasswd<br /><br />

---+++ CRAB Utility

a utility to submit CMSSW jobs to distributed computing resources<br />https://twiki.cern.ch/twiki/bin/view/CMSPublic/SWGuideCrab<br />CRAB Tutorial<br />https://twiki.cern.ch/twiki/bin/view/CMSPublic/CRAB3AdvancedTutorial<br />CRAB FAQ<br />https://twiki.cern.ch/twiki/bin/view/CMSPublic/SWGuideCrabFaq<br />CRAB job errors:<br />https://twiki.cern.ch/twiki/bin/view/CMSPublic/JobExitCodes<br />CRAB Commands:<br />https://twiki.cern.ch/twiki/bin/view/CMSPublic/CRAB3Commands<br /><br />There is ONE Tier0 site: <br />1. CERN<br />Seven T1 sites: <br />1. USA<br />2. France<br />3. Spain<br />4. UK<br />5. Taiwan<br />6. Germany <br />7. Italy<br />~55 T2 sites<br />You must specify config.Site.storageSite, which will depend on which center is hosting your area, and user_remote_dir which is the subdirectory of /store/user/ you want to write to. <br />* Caltech storage_element = T2_US_Caltech <br />* Florida storage_element = T2_US_Florida <br />* MIT storage_element = T2_US_MIT <br />* Nebraska storage_element = T2_US_Nebraska <br />* Purdue storage_element = T2_US_Purdue <br />* UCSD storage_element = T2_US_UCSD <br />* Wisconsin storage_element = T2_US_Wisconsin <br />* FNAL storage_element = T3_US_FNALLPC <br /><br />Check out all the tiers here:<br />https://cmsweb.cern.ch/sitedb/prod/sites<br /><br /><br />You need a CRAB config file in order to run an MC event generation code.<br />- The cmsDriver.py tool helps to generate config files<br />- examples of crab_cfg.py files:<br />crab_GEN-SIM.py<br />crab_PUMix.py<br />crab_AODSIM.py<br />crab_MINIAODSIM.py<br /><br />A typical CRAB config file looks like:<br />====================================<br />from WMCore.Configuration import Configuration<br />config = Configuration()<br /><br />config.section_("General")<br />config.General.requestName = 'CMSDAS_Data_analysis_test0'<br />config.General.workArea = 'crab_projects'<br /><br />config.section_("JobType")<br />config.JobType.pluginName = 'Analysis'<br />config.JobType.psetName = 'slimMiniAOD_data_MuEle_cfg.py'<br />config.JobType.allowUndistributedCMSSW = True<br /><br />config.section_("Data")<br />config.Data.inputDataset = '/DoubleMuon/Run2016C-03Feb2017-v1/MINIAOD'<br />config.Data.inputDBS = 'global'<br />config.Data.splitting = 'LumiBased'<br />config.Data.unitsPerJob = 50<br />config.Data.lumiMask = 'https://cms-service-dqm.web.cern.ch/cms-service-dqm/CAF/certification/Collisions16/13TeV/Cert_271036-275783_13TeV_PromptReco_Collisions16_JSON.txt'<br />config.Data.runRange = '275776-275782'<br /><br />config.section_("Site")<br />config.Site.storageSite = 'T2_US_Florida'<br />====================================<br /><br />The /store/user/ area at LPC is commonly used for the output storage from CRAB jobs<br /><br />How to make CRAB commands available: (must be in CMSSW environment)<br />cmsenv<br />source /cvmfs/cms.cern.ch/crab3/crab.sh      #.csh for c-shells<br /><br />To check that it worked successfully, do:<br />which crab<br />&gt; /cvmfs/cms.cern.ch/crab3/slc6_amd64_gcc493/cms/crabclient/3.3.1707.patch1/bin/crab<br />or:<br />crab --version<br />&gt; CRAB client v3.3.1707.patch1<br /><br />crab checkusername<br />&gt; Retrieving username from SiteDB...<br />&gt; Username is: drosenzw<br /><br />Can also test your EOS area grid certificate link:<br />crab checkwrite --site=T3_US_FNALLPC   &lt;== checks to see if you have write permission at FNAL<br />crab checkwrite --site=T2_US_Nebraska<br />crab checkwrite --site=T2_US_Florida<br /><br />***N.B. It is better to use: low num jobs, high num events/job!***<br /><br />First you can run a job locally, to make sure all is well:<br />cmsRun &lt;step1_cfg.py&gt;<br /><br />Submitting CRAB job:<br />crab submit -c crabConfig_MC_generation.py<br /><br />Resubmitting a CRAB job:<br />crab resubmit --siteblacklist='T2_US_Purdue' &lt;crabdir1&gt;/&lt;crabdir2&gt;   &lt;== don't submit to Purdue<br />- N.B. only failed jobs get resubmitted<br />- There are lots of flags to call to change things like memory usage, priority, sitewhitelist, etc.<br />- --sitewhitelist=T2_US_Florida,T2_US_MIT<br />- Can also use wildcards: --siteblacklist=T1_*<br /><br />Resubmitting SPECIFIC CRAB jobs:<br />crab resubmit --force --jobids=1,5-10,15 &lt;crabdir1/crabdir2&gt;   &lt;== N.B. you must --force successful jobs to resubmit<br /><br />Check number of events from CRAB job:<br />crab report &lt;crab_DIR&gt;/&lt;crab_job&gt;<br /><br />Check status:<br />crab status<br />crab status &lt;crab_DIR&gt;/&lt;crab_job&gt;<br /><br />Kill a job:<br />crab kill -d &lt;crab_DIR/crab_job&gt;<br /><br />For help with MC generation (step1):<br />https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookCRAB3Tutorial#2_CRAB_configuration_file_to_run<br /><br />
---+++ LXBATCH

https://twiki.cern.ch/twiki/bin/view/Main/BatchJobs<br />http://www.slac.stanford.edu/exp/atlas/computing/batchDesc.html<br /><br />echo "cd $CMSSW_BASE &&" `eval $(scram ru -sh)` "cd $(pwd) && ./JHUGen ....." | bsub -q 1nd -J jobname<br /><br />Useful commands:<br />bjobs [-l]      &lt;== check job status<br />bpeek      &lt;== check stdout so far<br />bkill &lt;job_id&gt;   &lt;== kill a job<br /><br />OPTIONS:<br />-c &lt;[hh:]mm&gt;      &lt;== sets CPU time limit, supposedly default is no-limit, but I don't trust that<br /><br />

---+++ CONDOR

https<br />CASTOR is a big storage space for lxplus<br /><br />3 MAIN FILES:<br />bash script      &lt;== a wrapper, "condor.sh", call the code that you want to run, kind of sets parameters<br />- e.g. condor.sh<br />submit script      &lt;== typical cluster parameters, memory, universe, queue, group<br />- condor.sub<br />DAG script      &lt;== contains the jobs, children, parents needed for condor<br />- this dag script is produced from a perl/bash command<br />- then condor reads this DAG script<br /><br />Run condor:<br />condor_submit &lt;submitfile.sub&gt;<br />condor_submit_dag &lt;file.dag&gt;   &lt;== this submits the dag file to condor (i.e. submits your jobs!)<br /><br />log/&lt;bunch_of_log_files&gt;<br />- one of which is:<br />   output.log      &lt;== has stdout from code that you want condor to process!<br /><br />Kill ALL jobs under your username:<br />condor_rm &lt;username&gt;      &lt;== <br />condor_q               &lt;==

---+++ CASTOR

https://twiki.cern.ch/twiki/bin/view/Main/HowtoUseLxplus#Helpful_linux_commands<br />CASTOR is a big storage space for lxplus<br /><br />New commands!<br />ls      ==&gt; nsls<br />mkdir   ==&gt;   nsmkdir<br />cp      ==&gt; rfcp<br />rm      ==&gt; rfrm<br />chmod   ==&gt; rfchmod

---++ LXPLUS

Your user area:   /afs/cern.ch/user/d/drosenzw      &lt;== 10 GB storage<br />Your work area:   /afs/cern.ch/work/d/drosenzw   &lt;== 100 GB storage
---+++ EOS Storage

Must be logged into the LPC machines (Fermilab) or on lxplus<br />https://uscms.org/uscms_at_work/computing/LPC/usingEOSAtLPC.shtml<br />Big storage area for big files<br /><br />Past Jake says: <b>DON'T store big files in EOS</b><br />They are easier to access from Tier2 on HiPerGator through UF: /cms/data/store/user/drosenzw/<br />- use uberftp or gfal-copy to access them<br />if on IHEPA, can store big files in /raid/raid{5,6,7,8,9}<br /><br />Only lxplus accounts can access EOS storage!<br /><br />See if you have an eos area on an LPC machine:<br />eosls -d /store/user/drosenzw/<br /><br />Tier2 Storage is better:<br />/cms/data/store/user/drosenzw/         &lt;== HiPerGator at UF. ONLY WRITABLE BY CRAB. Output of CRAB stored here.<br />/cms/data/store/user/t2/users/rosedj1/      &lt;== HPG at UF. Put NTuples here.<br /><br />There are different eos storage areas:<br />/eos/uscms/store/user/drosenzw/         &lt;== My allocated EOS area. LPC's Tier3 eos storage (also: /store/user/drosenzw/ ). Use: eosls<br />/eos/cms/                        &lt;== lxplus<br />/eos/user/d/drosenzw/               &lt;== easily accessible from lxplus. SWAN also uses this<br />/uscms_data/d1/drosenzw/            &lt;== normal LPC area<br />/eos/uscms_data/d1/drosenzw/         &lt;== What even is this?<br /><br />MAIN COMMANDS:<br />On lxplus, do: <br />ls /eos/cms/<br />ls -l /eos/user/d/drosenzw/ <br />mkdir /eos/user/d/drosenzw/&lt;new_dir&gt;<br />eos ls -l /eos/user/d/drosenzw/            &lt;== different kind of listing?<br />eos mkdir /eos/user/d/drosenzw/&lt;new_dir_path&gt;   &lt;== different kind of mkdir?<br />xrdcp &lt;source_file&gt; root://eosuser.cern.ch//eos/user/d/drosenzw/&lt;new_file_name&gt; &lt;== copy files   <br />- SWAN is also connected to /eos/user/d/drosenzw/SWAN_projects<br /><br />Set up your environment:<br />export EOS_MGM_URL=root://eoscms.cern.ch <br /><br />File Names:<br />MGM:            root://cmseos.fnal.gov/<br />LFN (shortcut name):   /store/user/drosenzw/<br />- the LFN is an alias which can be used at ANY site (The LFN is Lenient, i.e. uses a short path like /store/user/...)<br />- the PFN is the actual file path <br /><br />eosquota<br />returns the amount of storage space used/available in personal EOS area<br /><br />`eosgrpquota lpctau’<br />checks the storage space for the group “lpctau”<br /><br />LISTING<br />`eosls /LFN’<br />lists files (NEVER USE `ls’!)<br /><br />`eosls -d /store/user/drosenzw/‘<br />lists directory entries<br />-l option for long listing<br />-a option for listing hidden entries<br /><br />DON’T USE WILDCARDS OR TAB-COMPLETION!<br />DON’T USE TRADITIONAL COMMANDS! ls, rm, cd, etc.<br /><br />COPYING<br />`xrdcp &lt;source_file&gt; root://cmseos.fnal.gov//store/user/drosenzw/newNameOfFile.txt' (Local file to EOS)<br />`xrdcp root://cmseos.fnal.gov//store/user/drosenzw/whateverFile.txt ~/newName.txt’ (EOS to local file)<br />`xrdcp root://cmseos.fnal.gov//store/user/drosenzw/whateverFile.txt \<br />? root://cmseos.fnal.gov//store/user/drosenzw/newFile.txt'<br />-f option can overwrite existing files<br />-s option for silent copy<br /><br />MAKE DIR<br />`eosmkdir /store/user/drosenzw/newDir’<br />-p option will make parent directories as needed<br />`eosmkdir /store/user/drosenzw/newDir1/newDir2/newDir3’<br /><br />REMOVING<br />`eosrm /store/user/drosenzw/EOSfile.txt’ - removes files<br />`eosrm -r /store/user/drosenzw/dir1’ - removes directory and all contents<br /><br />if you get scram b errors, first run:<br />cmsrel CMSSW_X_Y_Z<br /><br />MUST set up environment in working directory (YOURWORKINGAREA):<br />cd ~/nobackup/YOURWORKINGAREA/CMSSW_9_3_2/src<br />cmsenv<br /><br />For condor batch jobs:<br />xrdcp outputfile.root root://cmseos.fnal.gov//store/user/username/outputfile.root <br />or <br />xrdfs root://cmseos.fnal.gov ls /store/user/username<br /><br />Attaching files:<br />root -l root://cmsxrootd.fnal.gov//store/user/jjesus/rootFile.root<br />or <br />TFile theFile = TFile::Open("root://cmsxrootd.fnal.gov//store/user/jjesus/rootFile.root");<br /><br />in IHEPA, you add root://cmsio5.rc.ufl.edu//store/user/&lt;filepath&gt; in the front<br />TFile::Open() instead of TFile(path,"READ")<br /><br />

---+++ LPC / Fermilab / CMSDAS

LPC Contact for CMS DAS problems<br />cmsdasatlpc@fnal.gov <br />USCMS T1 Facility Support Team<br />uscms-t1@fnal.gov<br />Fireworks Problems:<br />fireworks-support@cernSPAMNOT.ch<br />Mattermost Problems:<br />service-desk@cern.ch<br /><br />Subscribe to hypernews: <br />https://hypernews.cern.ch/HyperNews/CMS/login.pl?&url=%2fHyperNews%2fCMS%2fcindex<br /><br />For CRAB Issues: CMSDASATLPC@fnal.gov<br /><br />Get Kerberos ticket: kinit UN@FNAL.GOV<br />to check: klist<br /><br />Log onto cmslpc-sl6 cluster:<br />ssh -Y &lt;UN&gt;cmslpc-sl6.fnal.gov<br />ssh -Y &lt;UN&gt;@cmslpcN.fnal.gov, where N is whatever node you want to join<br /><br />Initialize your proxy: <br />voms-proxy-init -voms cms --valid 168:00 (makes the proxy valid for a week instead of a day!)<br />source /cvmfs/cms.cern.ch/cmsset_default.sh   &lt;== or put this in .bash_profile<br /><br />Storage Areas:<br />/uscms/homes/d/drosenzw   &lt;== 2 GB storage area<br />/nobackup/            &lt;== larger mass storage area<br /><br />For DAS, each time I log into the sl6 cluster, I need to:<br />cd ~/nobackup/YOURWORKINGAREA/CMSSW_10_2_0/src<br />cmsenv<br /><br />Switch default shell from tcsh to bash:<br />To permanently change your default login shell, use the LPC Service Portal, login with your Fermilab Services username and password. Choose the " Modify default shell on CMS LPC nodes" ticket and fill it out.<br /><br />***<br />If you want to get the nice command line after a switch to bash, put source /etc/bashrc in your cmslpc ~/.bash_profile file<br />***<br /><br /><br />For help with Fireworks, contact Basil Schneider: basil.schneider@cern.ch<br /><br />I may still have issues pushing to GitHub. <br /><br />Keep getting this error in ROOT plots: <br />AutoLibraryloader::enable() and AutoLibraryLoader.h are deprecated.<br />Use FWLiteEnabler::enable() and FWLiteEnabler.h instead<br />Info in &lt;TCanvas::MakeDefCanvas&gt;: created default TCanvas with name c1<br /><br />FWLite (found in PhysicsTools):<br />Frame Work Lite is an interactive analysis tool integrated with the CMSSW EDM (Event Data Model) Framework. It allows you to automatically load the shared libraries defining CMSSW data formats and the tools provided, to easily access parts of the event in the EDM format within ROOT interactive sessions. It reads produced ROOT files, has full access to the class methods and there is no need to write full-blown framework modules. Thus having FWLite distribution locally on the desktop one can do CMS analysis outside the full CMSSW framework.<br /><br />Example command:<br />FWLiteHistograms inputFiles=slimMiniAOD_MC_MuEle.root outputFile=ZPeak_MC.root maxEvents=-1 outputEvery=100<br /><br />Fireworks: turns EDM collections into visual representations… i.e., turns .root files into event displays!<br />cmsShow DoubleMuon <i>n100.root<br />cmsShow --no-version-check root://cmseos.fnal.gov//store/user/cmsdas/2017/pre_exercises/DYJetsToLL.root<br /><br /><br />F</i>or help with: <br />process.maxEvents = cms.untracked.PSet<br />- https://twiki.cern.ch/twiki/bin/view/CMSPublic/SWGuidePoolInputSources<br /><br />An Event is a C++ object container for all RAW and reconstructed data related to a particular collision.<br /><br /><br />

---+++ DAS (Data Aggregation Service)

Big database to hold MC and data samples<br />https://cmsweb.cern.ch/das/<br />FAQ:<br />https://cmsweb.cern.ch/das/faq<br />Examples:<br />https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookDataSamples<br /><br />/object_type/campaign/datatier <br />or<br />/Primary/Processed/Tier<br />/&lt;primary-dataset&gt;/&lt;CERN-username_or_groupname&gt;-&lt;publication-name&gt;-&lt;pset-hash&gt;/USER<br /><br />Given a file, DAS can return a dataset!<br />Given a dataset, DAS can return all the associated files.<br /><br />Datasets (whether MC or actual data) are published on DAS<br />- A dataset is comprised of many root files<br />- Find the name of a dataset based on the file name:<br />dataset file=/store/relval/CMSSW_10_2_0/RelValZMM_13/MINIAODSIM/PUpmx25ns_102X_upgrade2018_realistic_v9_gcc7-v1/10000/3017E7A1-178D-E811-8F63-0025905A6070.root<br />&gt;&gt;&gt; /RelValZMM_13/CMSSW_10_2_0-PUpmx25ns_102X_upgrade2018_realistic_v9_gcc7-v1/MINIAODSIM<br /><br />If you have trouble finding a file that you KNOW is on DAS:<br />- change the dbs instance to something other than global, e.g. "prod/phys03"<br /><br />Example DAS Searches:<br />dataset release=CMSSW_9_3_0_pre5 dataset=/RelValZMM*/*CMSSW_9_3_0*/MINIAOD* <br />dataset release=CMSSW_10_2_0 dataset=/RelValZMM*/*CMSSW_10_2_0*/MINIAOD*<br />dataset=/DoubleMu*/*Run2017C*/MINIAOD*   &lt;== /object_type/campaign/datatier (/Primary/Processed/Tier)<br /><br />Can search for datasets from the command line using dasgoclient:<br />dasgoclient --query="dataset=/DoubleMuon*/Run2018A-PromptReco-v1/MINIAOD" --format=plain<br />- must first do: voms-proxy-init -voms cms<br /><br /><br />Get the LFN of a dataset by doing a DAS search, like:<br />file dataset=/GenericTTbar/HC-CMSSW_5_3_1_START53_V5-v1/GEN-SIM-RECO<br />which will retrieve the following LFN:<br />/store/mc/HC/GenericTTbar/GEN-SIM-RECO/CMSSW_5_3_1_START53_V5-v1/0010/00CE4E7C-DAAD-E111-BA36-0025B32034EA.root<br /><br />MCM (Monte Carlo Manager)<br />Not the same thing as DAS!<br />Use /mcm/ to find the correct info to find MC samples on DAS:<br />/mcm/ is the bookkeeping of all produced MC samples<br />- tells you details of how the MC samples were produced<br />- e.g., tells you location of makecards.sh and data sets<br />Put into mcm:<br />GluGluHToZZTo4L_M125_13TeV_powheg2_JHUGenV7011_pythia8<br /><br />MCM tutorial with David:<br />Did a search on DAS:<br />/GluGluHToZZ*4L*125*/*Fall17*94X*/MINIAODSIM<br />/Primary/Processed/Tier<br /><br />David noticed that the location of the MC files couldn't be found here.<br />So then he checked mcm (Monte Carlo Manager):<br />David's MCM user profile: https://cms-pdmv.cern.ch/mcm/users?prepid=dsperka&page=0&shown=51<br />- click: Request &gt; Navigation &gt; dataset_name<br />   - Here, type in the name of the dataset from DAS (without leading forward slash!)<br />e.g. GluGluHToZZTo4L _M125_13TeV_powheg2_JHUGenV7011_pythia8<br />- may have to click: Select view &gt; Fragment <br />- Then go back to Navigation and scroll to right to click the "enlarge" button<br />- This will bring up important information from "rawGitHub" about the MC samples<br />Of these, most notably is:<br />https://cms-pdmv.cern.ch/mcm/public/restapi/requests/get_fragment/HIG-RunIIFall17wmLHEGS-00607/0<br />- It has "Links to cards" - these are MC generation cards<br />- may have to manually search for a specific URL to get the &lt;whatev&gt;template.input:<br />https://raw.githubusercontent.com/cms-sw/genproductions/fd7d34a91c3160348fd0446ded445fa28f555e09/bin/Powheg/production/2017/13TeV/Higgs/gg_H_ZZ_quark-mass-effects_NNPDF31_13TeV/gg_H_ZZ_quark-mass-effects_NNPDF31_13TeV_template.input<br /><br />—————————————<br />EDM Utilities<br /><br />Commands:<br />edmFileUtil - uses a LFN (logical file name, an alias used for any site) to find a PFN (physical file name, actual file path)<br />edmDumpEventContent - see what class names etc. to use in order to access the objects in the MiniAOD file<br />edmProvDump - Prov=Provenance (origin, source, history), one can print out all the tracked parameters used to create the data file. For example, one can see which modules were run and the CMSSW version used to make the MiniAOD file.<br />edmEventSize - determines size of different branches<br /><br />e.g.<br />For any of the below commands, you can do: --help <br />edmFileUtil -d /store/relval/CMSSW_10_2_0/RelValZMM_13/MINIAODSIM/PUpmx25ns_102X_upgrade2018_realistic_v9_gcc7-v1/10000/3017E7A1-178D-E811-8F63-0025905A6070.root<br />&gt;&gt;&gt; root://cmsxrootd-site.fnal.gov//store/relval/CMSSW_10_2_0/RelValZMM_13/MINIAODSIM/PUpmx25ns_102X_upgrade2018_realistic_v9_gcc7-v1/10000/3017E7A1-178D-E811-8F63-0025905A6070.root<br /><br />edmDumpEventContent --all --regex slimmedMuons root://cmsxrootd.fnal.gov//store/user/cmsdas/2019/pre_exercises/0EE14BA8-41BB-E611-AD2F-0CC47A4D760A.root <br />&gt;&gt;&gt; spits out info about class names to access objects in MiniAOD file<br />- the columns are: (1) type of data, (2) module label, (3) product instance label, (4) process name<br /><br />edmProvDump root://cmseos.fnal.gov//store/user/cmsdas/2019/pre_exercises/0EE14BA8-41BB-E611-AD2F-0CC47A4D760A.root &gt; EdmProvDump.txt<br /><br />edmEventSize -v `edmFileUtil -d /store/user/cmsdas/2019/pre_exercises/0EE14BA8-41BB-E611-AD2F-0CC47A4D760A.root` &gt; EdmEventSize.txt <br />edmEventSize -v slimMiniAOD_MC_MuEle.root<br /><br />In a MiniAOD file, each event only contains about 30-50 KB of data.<br />The main contents of the MiniAOD are: <br />* High level physics objects (leptons, photons, jets, ETmiss), with detailed information in order to allow e.g. retuning of identification criteria, saved using PAT dataformats. &#x2028;Some preselection requirements are applied on the objects, and objects failing these requirements are either not stored or stored only with a more limited set of information. &#x2028;Some high level corrections are applied: L1+L2+L3(+residual) corrections to jets, type1 corrections to ETmiss. <br />* The full list of particles reconstructed by the ParticleFlow, though only storing the most basic quantities for each object (4-vector, impact parameter, pdg id, some quality flags), and with reduced numerical precision; these are useful to recompute isolation, or to perform jet substructure studies. &#x2028;For charged particles with pT &gt; 0.9 GeV, more information about the associated track is saved, including the covariance matrix, so that they can be used for b-tagging purposes. <br />* MC Truth information: a subset of the genParticles enough to describe the hard scattering process, jet flavour information, and final state leptons and photons; GenJets with pT &gt; 8 GeV are also stored, and so are the other mc summary information (e.g event weight, LHE header, PDF, PU information). &#x2028;In addition, all the stable genParticles with mc status code 1 are also saved, to allow reclustering of GenJets with different algorithms and substructure studies. <br />* Trigger information: MiniAOD contains the trigger bits associated to all paths, and all the trigger objects that have contributed to firing at least one filter within the trigger. In addition, we store all objects reconstructed at L1 and the L1 global trigger summary, and the prescale values of all the triggers.

---+++ svn

<span>"subversion" - seems like the lxplus version of git and version control<br />Use this to edit Analysis Notes in CMS<br /><br />Excellent tutorial on svn:<br />http://cmsdoc.cern.ch/cms/cpt/tdr/notes_for_authors_temp.pdf<br /><br />https://twiki.cern.ch/twiki/bin/view/Main/HowtoNotesInCMS<br /><br />https://twiki.cern.ch/twiki/bin/viewauth/CMS/Internal/TdrProcessing<br /><br />To get your AN/paper started:<br />svn co -N svn+ssh://svn.cern.ch/reps/tdr2 myDir<br />cd myDir<br />svn update utils<br />svn update -N [papers|notes]         &lt;=</span> choose one, papers or notes<br />svn update [papers|notes]/XXX-YY-NNN   &lt;== enter your AN or paper code<br />eval `[papers|notes]/tdr runtime -sh`   <br /><br />To modify:   <br />cd [papers|notes]/XXX-YY-NNN/trunk   <br /><br />To build the document:<br />tdr --style=pas b XXX-YY-NNN      # --style=paper for papers<br /><br /><br />Git-like commands to update files:<br />svn add &lt;newfiles&gt;      &lt;== YOU ONLY NEED TO DO THIS ONCE FOR ANY FILE<br />svn commit -m '&lt;Commit message&gt;'   &lt;== This will update the file<br /><br />svn status <br />svn status -u (--show-updates)<br /><br />Figures should reside in the fig/ directory<br />Figure &#x303;\ref{fig:test} shows a figure prepared with the TDR<br />template and illustrates how to include a picture in a document<br />and refer to it using a symbolic label.<br />\begin{figure}[!Hhtb]<br />\centering<br />\includegraphics{width=0.55\textwidth}{c1_BlackAndWhite}<br />\caption[Caption for TOC]{Test of graphics inclusion.\label{fig:test}}<br />\end{figure}<br />The result of the above is roughly as follows:<br />Figure 1 shows a figure prepared with the TDR template and illustrates how to<br />include a picture in a document and refer to it using a symbolic label.<br /><br />Colour versions of figures can by provided for PDF output using the combinedfigure macro in place of the \ includegraphics<br />command. This takes two arguments corresponding re-<br />spectively to the black and white and the coloured versions of the same picture, for example:<br />Figure &#x303;\ref{fig:test} shows a figure prepared with the TDR<br />template and illustrates how to include a picture in a document<br />and refer to it using a symbolic label.<br />\begin{figure}[!Hhtb]<br />\centering<br />\combinedfigure{width=0.4\textwidth}{c1_BlackAndWhite}{c1_Colour}<br />\caption[Caption for TOC]{Test of graphics inclusion.\label{fig:test}}<br />\end{figure}<br /><br />the recommended procedure is to use multiple instances of the<br />\includegraphics command, combined with the tabular environment if needed.<br /><br /><br />Lucien ditched svn and switched to git for our AN-18-194.<br />https://twiki.cern.ch/twiki/bin/viewauth/CMS/Internal/TdrProcessing<br /><br />Compare what version of the AN you have:<br />git log      &lt;== shows recent commits<br /><br />

---+++ Certificate Stuff:

Followed instructions on:
https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookStartingGrid#ObtainingCert
Anytime you want to access data on a TCreate a temporary proxy:
voms-proxy-init --rfc --voms cms
voms-proxy-init --voms cms --valid 168:00       &lt;== makes the proxy valid for a week instead of a day!
voms-proxy-init -debug

voms-proxy-info      &lt;== check your info

When your grid certificate expires, you get an error like:
“Error during SSL handshake:Either proxy or user certificate are expired.”

Request a new grid user certificate:
https://ca.cern.ch/ca/help/?kbid=024010

Must have these permissions:
<verbatim class="cmd">
usercert.pem -rw-r--r-- %BR%
userkey.pem -r-------- </verbatim>

So do:
<verbatim class="cmd">
chmod 644 path/to/usercert.pem
chmod 400 path/to/userkey.pem</verbatim>

Put certificate into browser and then into VOMS:
https://twiki.cern.ch/twiki/bin/view/CMSPublic/SWGuideVomsFAQ

Also need to link certificate to your account in SiteDB:
https://resources.web.cern.ch/resources/Manage/Accounts/MapCertificate.aspx
Check if it worked:
https://cmsweb.cern.ch/sitedb/prod/mycert

Potentially Useful:
https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookStartingGrid#ObtainingCert
https://twiki.cern.ch/twiki/bin/viewauth/CMS/SiteDBForCRAB

*Successfully registered for VO CMS membership on 2018-06-26*
*Able to submit crab jobs on 2018-07-17*

Your certificate subject (DN):
/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=*yourusername*/CN=820970/CN=&lt;your_name&gt;
/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=drosenzw/CN=820970/CN=Jake Rosenzweig  &lt;== NEW
The CA that issued your certificate:
/DC=ch/DC=cern/CN=CERN Grid Certification Authority

---+++ Local System

In case you ever get this kind of error:
“Could not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/private/var/folders/zj/mnvc1p6542bgc5j7npt_2jkh0000gn/T/pip-install-1uj6p02b/tabula-py/tabula/tabula-1.0.2-jar-with-dependencies.jar'
Check the permissions.”

This is because Homebrew doesn’t play nicely with pip. So do:
`python -m pip install --user --install-option="--prefix=" ’

If you ever get the following error:
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
Someone could be eavesdropping on you right now (man-in-the-middle attack)!
Then the simple fix is:
ssh-keygen -f ~/.ssh/known_hosts -R <hostname or ip address>
- for example, <hostname> = lxplus.cern.ch

---+++ Version Control

GitHub<br />BitBucket<br />svn<br /><br />A good collaborative workflow:<br />1. Fork the group's repo so that you have your own repo.<br />2. Make your own, new branch in the forked repo that you can work on.<br />3. Keep the master branch of the forked repo synched up with the group's master branch. (upstream)<br /><br />git config --global user.name [Name]<br />git config --global user.email [Email]<br />git config --global user.github [Account]<br />git config --global core.editor [your preferred text editor]<br /><br />Make the print log easier to read:<br />git config --global alias.lol 'log --graph --decorate --pretty=oneline --abbrev-commit'<br /><br />Pull a specific file from the GitHub repo:<br />git fetch                  &lt;== downloads all the recent changes, but it will not put it in your current <br />                        checked out code (working area).<br />git fetch origin <br /><br />git cherry-pick &lt;commit-ID&gt;      &lt;== grab the files from a specific commit(?)<br /><br />git checkout origin/master -- &lt;path/to/file&gt;<br />//git checkout &lt;local repo name(default is origin)&gt;/&lt;branch name&gt; -- path/to/file will checkout the particular file from the downloaded changes (origin/master).<br /><br />If a Core folder gets updated, do:<br />git submodule init   &lt;== may not need to do this every time<br />git submodule update<br /><br />If you need to pull down more recent code from a repo, you can stash your current changes:<br />git stash         &lt;== saves your modifications for later (so now you can: git pull)<br />git stash apply   &lt;== brings those saved modifications back to life!<br /><br />Remotes:<br />Add a remote called "upstream" to push to original (not forked) repo:<br />git remote add upstream git@github.com:GitHATSLPC/GitHATS.git<br />- this is equivalent to doing:<br />      git fetch upstream master<br />      git merge upstream/master<br /><br />To keep from having to put in your password each time you push:<br />git remote show origin   &lt;== This shows you your repo_name<br />git remote set-url origin git+ssh://git@bitbucket.org/&lt;username&gt;/&lt;repo_name&gt;.git<br />Can also remove remotes:<br />git remote rm origin<br /><br />Two ways to make a repo:<br />1. Create repo in terminal:<br />   1. git init<br />   2. git add .<br />   3. git commit -m 'Commit message’<br />       1. undo with: git reset --soft HEAD~1<br />   4. git remote add origin &lt;remote_repo_URL&gt;<br />       1. GitHub:    git@github.com:&lt;user_name&gt;/&lt;repo_name&gt;.git   &lt;== make &lt;repo_name&gt; whatever you want!<br />       2. bitbucket: https://username@your.bitbucket.domain:7999/yourproject/repo.git<br />   5. git push -u origin master<br /><br />2. Create repo online and clone into terminal:<br />   1. make repo on BitBucket or GitHub <br />   2. git clone &lt;remote_repo_URL&gt;<br /><br /><br />Check the status of latest changes in your own repo:<br />git status<br />git status -s   &lt;== short format<br />Also useful:<br />git diff      &lt;== shows edits between old and new files, line by line<br />git diff &lt;file&gt;   &lt;== specifically, compares the changes you have made to last committed version of file<br /><br />If you get the following error:<br />error: The requested URL returned error: 403 Forbidden while accessing https://github.com/rosedj1/AN-18-194/info/refs<br />Then do:<br />1. edit .git/config file under your repo directory<br />2. find url=entry under section [remote "origin"]<br />3. change it from <br />url=https://MichaelDrogalis@github.com/derekerdmann/lunch_call.git to url=ssh://git@github.com/derekerdmann/lunch_call.git <br />that is, change all the texts before @ symbol to ssh://git<br /><br /><br />True = use_syst ! Enable systematics studies<br /><br /><br /><br />USEFUL!<br />To remove a file from your remote git repo:<br />git rm &lt;file1&gt;               &lt;== I think this also deletes the file locally!<br />git rm --cached &lt;file1&gt;         &lt;== does NOT delete file locally; only on the remote repo!<br />then do:<br />git commit -m "removing &lt;file1&gt;"<br />git push origin &lt;branch_name&gt;<br /><br />Remove a directory:<br />git rm -r &lt;dir1&gt;<br /><br />Say you have made a pull request and a bunch of commits which you can see on GitHub.<br />Now you want to remove those files from the PR. <br />Doing rm &lt;file&gt; from your local computer won't take it away from GitHub.      &lt;== may not be true<br />So you can REMOVE previously committed files by doing:               &lt;== also may not be true<br />git rm &lt;file&gt;<br /><br />If you have a file in a PR that you want to delete, or say you have sensitive info in a PR<br />which must be deleted, you should 'rewrite' the commit:<br />git commit --amend         &lt;== just do this if your most recent commit is local (not online)<br />git push --force origin &lt;branch&gt;   &lt;== otherwise, include this part too to rewrite the history online<br /><br />If you move a repo to a new location:<br />git remote set-url origin ssh://git@gitlab.cern.ch:7999/cms-rcms-artifacts/gitlab-maven.git<br />or<br />git remote set-url origin https://gitlab.cern.ch/cms-rcms-artifacts/gitlab-maven.git<br /><br /><br />GitHub Markdown:<br />*&lt;words&gt;*   or   _&lt;words&gt;_   &lt;== make &lt;words&gt; italic      (called "emphasis")<br />**&lt;words&gt;**   or   __&lt;words&gt;__   &lt;== make &lt;words&gt; bold      (called "strong emphasis")<br />**&lt;words&gt; and _&lt;newwords&gt;_**   &lt;== &lt;words&gt; and &lt;newwords&gt;   (called "combined emphasis")<br />~~&lt;words&gt;~~            &lt;== make &lt;words&gt; strikethrough<br />{code}&lt;words&gt;{code}   &lt;== make &lt;words&gt; monospace and code-like<br />!!&lt;space&gt;         &lt;== make entire message monospace by beginning message with '!!' and then a space!<br />@@&lt;space&gt;         &lt;== ignore all special formatting by beginning message with '@@' and then a space<br /><br />Code<br />`&lt;code&gt;`         &lt;== inline &lt;code&gt;<br /><br />```python<br />&lt;code&gt;<br />```            &lt;== block &lt;code&gt; with python syntax highlighting<br /><br />Headers<br /># H1         &lt;== biggest text<br />## H2<br />### H3<br />#### H4<br />##### H5   &lt;== smallest text<br />###### H6   &lt;== smallest text, but greyed out<br /><br />Lists<br />1. First ordered list item<br />2. Another item<br />&sdot;&sdot;* Unordered sub-list. <br />1. Actual numbers don't matter, just that it's a number<br />&sdot;&sdot;1. Ordered sub-list<br />&sdot;&sdot;1. Second item in the sub-list. Remember, GitHub Markdown has automatic numbering<br />4. And another item.<br /><br />&sdot;&sdot;&sdot;You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we'll use three here to also align the raw Markdown).<br /><br />&sdot;&sdot;&sdot;To have a line break without a paragraph, you will need to use two trailing spaces.&sdot;&sdot;   &lt;== two trailing spaces keeps you in same paragraph<br />&sdot;&sdot;&sdot;Note that this line is separate, but within the same paragraph.&sdot;&sdot;<br /><br />Unordered Lists<br />* Unordered list can use asterisks<br />- Or minuses<br />+ Or pluses<br /><br />Tables<br />| Tables       | Are          | Cool |<br />| ------------- |:-------------:| -----:|<br />| col 3 is     | right-aligned | $1600 |<br />| col 2 is     | centered     |  $12 |<br />| *zebra stripes* | `are neat`     |   $1 |<br /><br />- Colons can be used to align columns.<br />- There must be at least 3 dashes separating each header cell.<br /><br /><br />Blockquotes         &lt;== look like quotes from a forum or email<br />&gt; &lt;quoted_text&gt;<br /><br /><br />Make a horizontal line (all methods are the same):<br />***   or   ___   or   ---   <br /><br />You can also add: Images, Hyperlinks, inline HTML, and YouTube videos<br /><br /><br /><br /><br />Git Tutorial with Matt Gitzendanner:<br />Matt's slides can be found at: <br />github.com/ufresearchcomputing/git-training<br />- It can actually host slides and presentations<br />- Could be used for writing text<br /><br />git-scm.com   &lt;== helps download and install git<br /><br />Master branch = main copy of the code, function code<br />Repository = collection of files, place where you want to organize projects<br />Working Copy --&gt; Staging Area --&gt; Commit to local repo<br /><br />Some commands:<br />git init      &lt;== turn current directory into a repo<br />git fetch      &lt;== goes between online repo and local repo<br />git checkout   &lt;== goes between local repo and working area<br />git merge      &lt;== goes between local repo and working area<br />git reset HEAD~1   &lt;== delete the last commit<br />git commit --amend   &lt;== Merge commits and commit again<br /><br /><br />BRANCHING<br />git branch newbranch<br />git checkout newbranch<br />(or just: git checkout -b newbranch)<br />git branch -a            &lt;== shows all local and remote branches<br />git branch -r            &lt;== only show remote branches<br />git branch -d branch_name   &lt;== delete local branch<br />git branch -D branch_name   &lt;== force delete local branch<br />git push &lt;remote_name&gt; --delete &lt;branch_name&gt;   &lt;== delete remote branch<br /><br />git push --set-upstream origin newbranch<br /><br />git reset HEAD &lt;file&gt;   &lt;== unstage files which have been added<br />git log   &lt;== shows changes you've made<br /><br />CMS has its own git init command: git cms-init<br />- be sure to do cmsenv first<br /><br />—————————————<br />IDEs and Interpreters<br /><br />SWAN<br />A Jupyter Notebook for CMSSW, ROOT, and PyROOT!<br /><br />Start a session:<br />https://swan001.cern.ch/hub/home<br /><br />You can jump straight into a Jupyter Notebook and/or jump into a terminal.<br />The terminal is linked to:<br />/eos/user/d/drosenzw/SWAN_projects/<br /><br />URL to access SWAN:<br />swan003.cern.ch<br />potential nodes: [1-5]<br /><br /><br /><br /><br />Jupyter Notebook Stuff<br />Go to: https://help.rc.ufl.edu/doc/Remote_Jupyter_Notebook<br />Copy and paste the SLURM script there.<br />sbatch &lt;SLURM_job.sh&gt;<br />localhost:&lt;port_number&gt;<br /><br />ssh -NL 26686:c25a-s26.ufhpc:26686 &lt;UN&gt;@hpg2.rc.ufl.edu<br /><br />module add jupyter<br />launch_jupyter_notebook<br /><br />In the job's log file, you will find the host <br /><br />module key &lt;keyword&gt;<br />module spider &lt;keyword&gt;<br /><br />There is a thing called: Jupyter lab<br />- There's also jhub (jupyter hub)<br /><br />Cmd + ]      &lt;== tab/indent<br />Cmd + [      &lt;== untab/dedent<br />Cmd + /      &lt;== (un)comment highlighted text<br />X         &lt;== delete selected cell<br />Z         &lt;== undo cell deletion<br /><br />Other cool tricks:<br />Click the left part of the output of a cell to contract and scroll through the output<br />- makes your Jupyter Notebook more manageable<br /><br /><br /><br />C++ IDE:<br />If you want a C++ IDE like Jupyter, check out:<br />xeus-cling   &lt;== never successfully worked...<br />binder      &lt;== kept crashing<br /><br />conda activate xeus<br />conda deactivate<br /><br />A command that starts with % is called a "line magic" and %% is called a "cell magic"<br />- these are non-native to C++ or python, but understood by the IDE for really cool effects!<br /><br />%%file &lt;filename&gt;      &lt;== writes over file &lt;filename&gt; with cell contents<br />%%file -a &lt;filename&gt;   &lt;== appends to &lt;filename&gt;\<br /><br /><br /><br />

---+++ LaTeX

%   &lt;== comments<br /><br />Table of Contents (ToC) are very easily built by LaTeX!<br /><br />Referencing<br />One section in a ToC may be in a file: sec-015-model.tex<br />Inside this file might be:<br />\section{Dark photon model}<br />\label{sec:model}<br />- when other pieces of code, like the ToC, need to reference this section <br />   (using something like: \ref{sec:model}), they need to know the label of the section!<br /><br /><br />

---+++ TWiki Markdown:

CMSNewcomerHandbook
https://twiki.cern.ch/twiki/bin/view/Sandbox/CMSNewcomerHandbook
Intro:
https://twiki.cern.ch/twiki/bin/view/TWiki/ATasteOfTWiki?slideshow=on;skin=print#GoSlide1
User guide:
https://twiki.cern.ch/twiki/bin/view/TWiki/TWikiUsersGuide
Markdown help:
https://twiki.cern.ch/twiki/bin/view/TWiki/TextFormattingRules    (long version)
https://twiki.cern.ch/twiki/bin/view/TWiki/TWikiShorthand       (short version)
TWiki variables:
https://twiki.cern.ch/twiki/bin/view/TWiki/TWikiVariables
TWiki plugins:
twiki.org/cgi-bin/view/Plugins

   * Easiest to edit the TWiki using Raw Edit.

Markdown:
_italics_
*bold*
__bold italic__
=monospace=
==bold monospace==

<verbatim class="cmd">
block of code</verbatim>

Disable formatted text:
<nop>*word*
!*word*

Separate paragraphs with a blank line

---+ &lt;== This is a heading
---++ &lt;== Deeper heading

---         &lt;== horizontal bar

%TOC{title="Goodies:"}%      &lt;== Table of Contents

   * text &lt;== three spaces, then * starts a bulleted list
    - (further bullets are indented via whitespace triplets)
   1 text &lt;== three spaces, then some number, starts a numbered list
    - doesn't matter what number you put!
    - Use the %BR% variable to add a paragraph without renumbering the list

| Cat | Dog
| boo | yah! |    &lt;== creates a table

%RED% your_text %ENDCOLOR      &lt;== color your text red

BumpyWord   &lt;== using CamelCase like this creates an auto-hyperlink to BumpyWord 's TWiki
[[BumpyWords][bumpy words]] appears as bumpy words
[[http://www.google.com/][Google]] appears as Google

%SEARCH   &lt;== This is an interface to a sophisticated search engine that embeds the results of the search in your page

Three kinds of documents on the TWiki:
1. DocumentMode = community property, anyone can edit
2. ThreadMode = Q&A
3. StructuredMode = has definite structure and rules to follow

Import an image:

<verbatim class="cmd"><img align="right" alt="CRAB Logo" src="http://cmsdoc.cern.ch/cms/ccs/wm/www/Crab/img/crab_logo_3.png" width="154" /> </verbatim>

---++ Important Particle Physics Stuff:

---++ MadGraph5_aMC@NLO

LO and NLO Monte Carlo event generator
   * "Mad" stands for *Mad*ison-Wisconsin!

*Excellent !MadGraph5 tutorial:* <br />
https://twiki.cern.ch/twiki/bin/view/CMSPublic/MadgraphTutorial

---

*How to install !MadGraph5 (MG5):*

   1 Go to *https://launchpad.net/mg5amcnlo* and right-click your favorite version of MG5.
   1 Click "Copy Link Location".
   1 Download MG5 to your shell by pasting in the link:
<verbatim class="cmd">wget https://launchpad.net/mg5amcnlo</verbatim>

Untar the downloaded tarball and display the contents:
<verbatim class="cmd">
tar -zxf MG5_aMC_v2.6.5.tar.gz
ls -l MG5_aMC_v2_6_5/
</verbatim>

You should see something like this:
<verbatim class="output">
total 7.4M
drwxr-xr-x.  4 drosenzw zh 2.0K Jul 12 00:49 Delphes
drwxr-xr-x.  3 drosenzw zh 2.0K Jul 12 00:49 ExRootAnalysis
drwxr-xr-x.  3 drosenzw zh 6.0K Jul 12 00:49 HELAS
drwxr-xr-x. 15 drosenzw zh 2.0K Jul 12 00:49 HEPTools
-rw-r--r--.  1 drosenzw zh 1.9K Jul 12 00:48 INSTALL
lrwxr-xr-x.  1 drosenzw zh   16 Jul 12 00:48 LICENSE -> madgraph/LICENSE
drwxr-xr-x.  3 drosenzw zh 2.0K Jul 12 00:48 MadSpin
drwxr-xr-x.  2 drosenzw zh 2.0K Jul 12 00:48 PLUGIN
-rw-r--r--.  1 drosenzw zh 2.2K Jul 12 00:49 README
drwxr-xr-x.  8 drosenzw zh 2.0K Jul 12 00:48 Template
-rw-r--r--.  1 drosenzw zh 122K Jul 12 00:48 UpdateNotes.txt
-rw-r--r--.  1 drosenzw zh   41 Jul 12 00:49 VERSION
drwxr-xr-x.  4 drosenzw zh 2.0K Jul 12 01:01 aloha
drwxr-xr-x.  2 drosenzw zh 2.0K Jul 12 00:49 apidoc
drwxr-xr-x.  2 drosenzw zh 2.0K Jul 12 00:48 bin
drwxr-xr-x.  2 drosenzw zh 2.0K Jul 12 00:49 doc
-rw-r--r--.  1 drosenzw zh 7.2M Jul 12 00:48 doc.tgz
drwxr-xr-x.  2 drosenzw zh 2.0K Jul 12 00:48 input
drwxr-xr-x. 10 drosenzw zh 2.0K Jul 12 01:01 madgraph
drwxr-xr-x.  2 drosenzw zh 2.0K Jul 12 01:01 mg5decay
drwxr-xr-x. 10 drosenzw zh 2.0K Jul 12 01:01 models
-rw-r--r--.  1 drosenzw zh 1.8K Jul 12 00:49 proc_card.dat
-rw-r--r--.  1 drosenzw zh    0 Jul 12 00:49 pythia-pgs.tgz
drwxr-xr-x.  7 drosenzw zh 2.0K Jul 12 00:48 tests
drwxr-xr-x.  7 drosenzw zh 2.0K Jul 12 01:01 vendor
</verbatim>

*Notes about a couple of these:*
   * The =models/= dir contains different theoretical models which MG5 can import.
      * Put new models inside this dir to successfully ==import model <new_model>==.
   * The =proc_card.dat= file contains the default process to be generated.
   * The =bin/= dir contains the executable =mg5_aMC=. Let's play with that next.

Boot up the MG5 interpreter:
<verbatim class="cmd">./MG5_aMC_v2_4_2/bin/mg5_aMC</verbatim>
Note that by default the Standard Model gets imported: 
<verbatim class="output">Loading default model: sm</verbatim>

See what particles MG5 currently knows about:
<verbatim class="cmd">display particles</verbatim>
Look at the particles with a little more detail:
<verbatim class="cmd">display multiparticles</verbatim>
Look at the possible vertices:
<verbatim class="cmd">display interactions</verbatim>

Generate a process:
<verbatim class="cmd">generate p p > l+ l-</verbatim>

Save this process in a *newly-created dir*:
<verbatim class="cmd">output <new_dir_name></verbatim>

Calculate the cross section of the process:
<verbatim class="cmd">launch</verbatim>
Then type =0=, and =0= again.
---
*MORE COMMANDS AND INFO*

Bring up the *help menu* or *help on a specific command*:
<verbatim class="cmd">
help
help <cmd>
</verbatim>

Syntax for generate:
generate INITIAL STATE &gt; REQ S-CHANNEL &gt; FINAL STATE $ EXCL S-CHANNEL / FORBIDDEN PARTICLES COUP1=ORDER1 COUP2=ORDER2 @N
Example:
l+ vl &gt; w+ &gt; l+ vl a $ z / a h QED=3 QCD=0 @1

Display information about current model:
display particles      &lt;== shows you particles
display interactions      &lt;== shows you possible vertices
display multiparticles   &lt;== shows you currently-defined multiparticles

Define new particles (or groups of particles):
define v = w+ w- z a   &lt;== defines 'v' to be any vector boson
define p = p b b~      &lt;== adds b bbar to current definition of proton

Import a new model:
import model mssm

Modify the model:
customize_model
- useful for setting a mass to zero, or removing some interaction, etc.
customize_model --save=&lt;new_model_name&gt;      &lt;== save new model

Save MG5 commands from interactive session
history &lt;my_mg5_cmd&gt;.dat

Execute commands stored in history file:
import command   &lt;my_mg5_cmd&gt;.dat      &lt;== from MG5 CLI
./bin/mg5_aMC   my_mg5_cmd.dat         &lt;== from your shell

Execute shell commands from MG5 CLI:
!<cmd>      &lt;== option 1
shell <cmd>   &lt;== option 2

Rerun a 'launch' command from a dir that was produced using 'output'
./bin/generate_events

./bin/madevent
do: pythia run_01

import model HAHM_variablesw_v3_UFO
define q = u d s c t b u~ d~ c~ s~ t~ b~
generate q q &gt; z z / g h h2 , z &gt; l+ l-
output <dirname>
launch <dirname>

Can specify the number of vertices you want:
generate p p &gt; h &gt; j j e+ e- vm vm~ QCD=0 QED=6

p p &gt; t t~          &lt;== gives only dominant QCD vertices; ignores QED vertices
p p &gt; t t~ QED=2   &lt;== gives both QCD and QED vertices

Add new processes to current process:
add process p p &gt; h &gt; j j mu+ mu- ve ve~ QCD=0 QED=6

Example generate processes:
generate p p &gt; h , (h &gt; hs hs, (hs &gt; zp zp, (zp &gt; l+ l-)))
generate p p &gt; h &gt; j j e+ e- vm vm~ QCD=0 QED=6
generate p p &gt; w+ w- / h      &lt;== exclude Higgs
generate p p &gt; w+ w- $ h   &lt;== exclude on-shell Higgs (but retain interference effects)

Export (save) processes:
output &lt;dir_name&gt;
- executing 'output' automatically writes the Feynman diagrams to the subprocess/matrix.ps file

Look at Feynman diagrams of loaded processes:
display diagrams

Tips:
- Usually good to put: ptj = 0.01 (= 0 has caused problems)
- qscale at ME level is controlled by ptj at NLO and by xqcut at LO
- draj = 0.05 (this is the deltaR between gamma and jets)
- jetradius = 0.7 (for non-FXFX merging samples)
- lhaid = 292000 (for 4 fermion final state)

Fix:
******Appending [QCD]    &lt;== applies NLO QCD corrections to process

generate p p &gt; w+, w+ &gt; ell+ vl @0   &lt;== '@0' is still leading order...

How to fix certain errors:
Error detected in "import model
Must put a "model dir" with all the model cards inside MG5_aMC_v2_6_5/models/
- a model dir has files like: "couplings.py", "vertices.py", "decays.py"

*For help on using MCM or php:* <br />
https://indico.cern.ch/event/807778/contributions/3362163/attachments/1826349/2989132/mccmTutorial.pdf

How to install LHAPDF sets:
Open up a MG5 interpreter and do:
install lhapdf6
BEWARE! IT'S NOT GUARANTEED TO WORK!
while doing 'install lhapdf6' some errors are encountered,
specifically that the desired dir is never created:
/20190422_HAHM_qqZZ4L/MG5_aMC_v2_6_5/HEPTools/lhapdf6/share/LHAPDF/

instead it only creates:
/20190422_HAHM_qqZZ4L/MG5_aMC_v2_6_5/HEPTools/lhapdf6/

Need to MANUALLY put these files into .../share/LHAPDF/:
- pdfsets.index
- lhapdf.conf
/cvmfs/cms.cern.ch/lhapdf/pdfsets/6.2/pdfsets.index

Then download the desired pdfs into .../share/LHAPDF/:
wget https://lhapdf.hepforge.org/downloads?f=pdfsets/6.1/NNPDF23_lo_as_0130_qed.tar.gz -O NNPDF23_lo_as_0130_qed.tar.gz
tar xvfz  NNPDF23_lo_as_0130_qed.tar.gz

If you want to view the code that fails:
MG5_aMC_v2_6_5/HEPTools/HEPToolsInstallers/installLHAPDF6.sh

value '230000' for entry 'pdlabel' is not valid.  Preserving previous value: 'nn23nlo'.
allowed values are lhapdf, cteq6_m, cteq6_d, cteq6_l, cteq6l1, nn23lo, nn23lo1, nn23nlo

Change Fortran compiler to "gfortran":
MG5_aMC_v2_6_5/input/mg5_configuration.txt

LHAPDF_DATA_PATH=/cvmfs/cms.cern.ch/lhapdf/pdfsets/6.2/NNPDF30_nlo_nf_5_pdfas
PATH
PYTHONPATH
LD_LIBRARY_PATH
/afs/cern.ch/work/d/drosenzw/DarkZ/MG5_gridpacks_practice/CMSSW_10_2_0/biglib/slc6_amd64_gcc700:/afs/cern.ch/work/d/drosenzw/DarkZ/MG5_gridpacks_practice/CMSSW_10_2_0/lib/slc6_amd64_gcc700:/afs/cern.ch/work/d/drosenzw/DarkZ/MG5_gridpacks_practice/CMSSW_10_2_0/external/slc6_amd64_gcc700/lib:/cvmfs/cms.cern.ch/slc6_amd64_gcc700/cms/cmssw/CMSSW_10_2_0/biglib/slc6_amd64_gcc700:/cvmfs/cms.cern.ch/slc6_amd64_gcc700/cms/cmssw/CMSSW_10_2_0/lib/slc6_amd64_gcc700:/cvmfs/cms.cern.ch/slc6_amd64_gcc700/cms/cmssw/CMSSW_10_2_0/external/slc6_amd64_gcc700/lib:/cvmfs/cms.cern.ch/slc6_amd64_gcc700/external/llvm/6.0.0-gnimlf2/lib64:/cvmfs/cms.cern.ch/slc6_amd64_gcc700/external/gcc/7.0.0-omkpbe2/lib64:/cvmfs/cms.cern.ch/slc6_amd64_gcc700/external/gcc/7.0.0-omkpbe2/lib:/cvmfs/cms.cern.ch/slc6_amd64_gcc700/external/cuda/9.2.88-gnimlf/drivers

Maybe need to do this:
export PATH=$PATH:&lt;MG_PATH&gt;/HEPTools/lhapdf6/bin
/afs/cern.ch/work/d/drosenzw/DarkZ/MG5_gridpacks_practice/HAHM_LO/HAHM_variablesw_v3/HAHM_variablesw_v3_gridpack/work/LHAPDF-6.2.1/bin

/afs/cern.ch/work/d/drosenzw/DarkZ/MG5_gridpacks_practice/HAHM_LO/HAHM_variablesw_v3/HAHM_variablesw_v3_gridpack/work/LHAPDF-6.2.1/bin/lhapdf

Les Houches Events (LHE) Files
When a LHE file is made, inside you will find something like this:
<verbatim class="cmd">
<event>
 8   1  0.6793200E-07  0.9147429E+02  0.7818608E-02  0.1356938E+00

<event>
 8   1  0.6793200E-07  0.9147429E+02  0.7818608E-02  0.1356938E+00
        2   -1    0    0  501    0  0.00000000000E+00  0.00000000000E+00  0.68369358717E+03  0.68369358717E+03  0.00000000000E+00 0. -1.
       -2   -1    0    0    0  501 -0.00000000000E+00 -0.00000000000E+00 -0.69562716985E+01  0.69562716985E+01  0.00000000000E+00 0.  1.
       23    2    1    2    0    0 -0.39165990322E+01  0.82379263440E+01  0.32833201641E+03  0.34083640999E+03  0.91018361847E+02 0.  0.
     1023    2    1    2    0    0  0.39165990322E+01 -0.82379263440E+01  0.34840529906E+03  0.34981344888E+03  0.29999890451E+02 0.  0.
      -11    1    3    3    0    0  0.17753759019E+02  0.35304624130E+01  0.31380369896E+03  0.31432534356E+03  0.51100000000E-03 0. -1.
       11    1    3    3    0    0 -0.21670358051E+02  0.47074639310E+01  0.14528317452E+02  0.26511066425E+02  0.51100000000E-03 0.  1.
      -11    1    4    4    0    0  0.85238449545E+01  0.88209596645E+01  0.94289826123E+02  0.95084365554E+02  0.51100000000E-03 0.  1.
       11    1    4    4    0    0 -0.46072459224E+01 -0.17058886009E+02  0.25411547294E+03  0.25472908333E+03  0.51100000000E-03 0. -1.</verbatim>

---++ UF-Specific Stuff

For potentially GREAT scripts, check out:
David:   /afs/cern.ch/user/d/dsperka/public/Run2MC/
Filippo:    /afs/cern.ch/user/f/ferrico/cmsonly/fitdymass.py

DON'T store big files in EOS
They are easier to access from Tier2: /cms/data/store/user/drosenzw/
- user uberftp or gfal-copy to access them
if on IHEPA, can store big files in /raid/raid{5,6,7,8,9}      &lt;== There's only 5-9!
- native raid areas:
/raid/raid5/   &lt;== gainesville
/raid/raid6/   &lt;== newberry
/raid/raid7/   &lt;== alachua
/raid/raid8/   &lt;== melrose             raid:  5      6     7    8   9
/raid/raid9/   &lt;== archer      (Mnemonic: Ga-New-Ala-M-Ar, pronounced "GNU Alamar")

Share files with others on lxplus:
/afs/cern.ch/work/<path>

Want to use Jupyter Notebooks on remote servers?
On remote server, do:
jupyter notebook --no-browser --port=8880
Then, on local computer:
ssh -N -L localhos8888:localhost:8880 rosedj1@melrose.ihepa.ufl.edu

For general tips on CMS analysis basics:
https://indico.cern.ch/event/759915/page/14801-pre-exercisesinstructions

For excellent code structure and interesting ideas like PyROOT and Jupyter Notebooks:
https://github.com/guitargeek/geeksw/tree/master/geeksw

Tips on how to be more efficient with code:
Use lots of print statements
sys.exit() to test sections
use the interpreter to test dummy examples
Talk about your code
Comment heavily at first, and then trim it down
When reading someone's code:
- Comment their code and explain it in plain English
- Start from some point that you the end and work your way backwards
- Just go line by line very carefully figuring out what each line does
- Try rewriting their code in chunks to reproduce what it is, but in your own way

---+++ UF Tier2 Commands (uberftp, xrootd, etc.)

A full list of IHEPA machines:
archer, alachua, melrose, newberry, or gainesville.ihepa.ufl.edu
ssh -X -Y lucien@newberry.ihepa.ufl.edu

Interact with UF Tier 2 storage directly
uberftp cmsio.rc.ufl.edu "ls /cms/data/store/user/drosenzw/"
uberftp cmsio.rc.ufl.edu "help" # To see available commands
uberftp cmsio.rc.ufl.edu "mkdir /cms/data/store/user/drosenzw/mynewdir/"
uberftp cmsio.rc.ufl.edu "rename /cms/data/store/user/drosenzw/olddir/ /cms/data/store/user/drosenzw/newdir/"

Copy files to/from UF Tier 2 storage
N.B. with gfal-copy, you CAN'T USE WILDCARDS! Just end with .../dir/

gfal-copy &lt;source_dir&gt; gsiftp://cmsio.rc.ufl.edu//cms/data/store/user/drosenzw/&lt;dest_dir&gt;

gfal-copy  -r  gsiftp://cmsio.rc.ufl.edu//cms/data/store/user/drosenzw/<filepath>    file:///home/rosedj1/DarkZ-EvtGeneration/CMSSW_9_3_1/src/DarkZ-EvtGeneration/workDir_DarkPhoton_mZd35/

T2 (HiPerGator):
My home path to which I can write:
/cms/data/store/user/t2/users/drosenzw/

t2_prefix = root://cmsio5.rc.ufl.edu/
Attach a file from T2
root -l root://cmsio5.rc.ufl.edu//store/user/drosenzw/rootfiles_2018/
f = TFile.Open("root://cmsio5.rc.ufl.edu//store/user/drosenzw/rootfiles_2018/UFHZZAnalysisRun2/Test3/","READ")

You may have to use one of the following paths instead
To access CMS data from IHEPA,
    please use root://cmsio5.rc.ufl.edu//store/...
            or root://cms-xrd-global.cern.ch//store/.. # access the file in any site
            or root://cms-xrd-global.cern.ch//store/test/xrootd/T2_US_Florida//store/...
             ( if cmsio5.rc.ufl.edu does not work for some reason )
               gsiftp://cmsio.rc.ufl.edu/cms/data/store/...

The best way to display plots on a website for easy viewing:
On IHEPA, do:
mkdir  -p  /home/<UN>/public_html/&lt;dest_dirs&gt;
cp  /home/rosedj1/index.php  /home/<UN>/public_html&lt;dest_dirs&gt;/

Then the plots will show up at this website:
http://tier2.ihepa.ufl.edu/~&lt;your_UN&gt;/

---+++ HiPerGator (HPG)

HiPerGator (HPG)

HiPerGator lectures given by Matt Gitzendanner

Find notes on HiPerGator (Find Matt Gitzendanner's presentations):
training.it.ufl.edu
Find SLURM commands at:
help.rc.ufl.edu
Interactive Jupyter Notebook session that uses HiPerGator!:
jhub.rc.ufl.edu

Location of SLURM example scripts:
/ufrc/data/training/SLURM/*.sh
- for single jobs, grab: single_job.sh
- for parallel jobs, grab: parallel_job.sh

You have a couple main directories:
/                  &lt;== where HPG first drops you off
/home/<gatorlink>/         &lt;== CANNOT handle big files (only has 20 GB of storage)
/ufrc/<group>/<gatorlink>   &lt;== can handle 51000 cores!

/ufrc/phz5155/$USER
- parallel file system
- CAN handle 51000 cores, reading and writing to it
- 2 TB limit per group
after ssh’ing into HPG, it will take you to:
/home/$USER
- for me this is: /home/rosedj1
- Get 20GB of space
- Has one server (node) hosting

My groups:
/ufrc/korytov/rosedj1/   &lt;== for particle physics research
/ufrc/phz5155/         &lt;== for computing course
- so I'm part of two different groups

To use class resources, instead of Korytov’s resources:
module load class/phz5155
- each time you want to submit a job, do this command^

It is useful to use the extension: .slurm for SLURM scripts

<verbatim class="cmd">
######################
## Basic SLURM job script:
#!/bin/bash
#SBATCH --job-name=test       # Name for job
#SBATCH -o job_%j.out          #
#SBATCH --mail-type=ALL
#SBATCH --mail-user=&lt;rosedj1@ufl.edu&gt;
#SBATCH --ntasks=1
#SBATCH --mem-per-cpu=100mb      or    #SBATCH --mem=1gb
#SBATCH --time=2:00:00 (hh:mm:ss)   or   #SBATCH -t=00:01:00

SCRIPT STUFF BELOW, e.g.
hostname
module load python
python -V
######################
</verbatim>

SLURM sbatch directives
multi-letter directives are double dashes:
--nodes=1   # processors
--ntasks
--ntasks-per-node
--ntasks-per-socket
--cpus-per-task (cores per task)
Memory usage:
--mem=1gb
--mem-per-cpu=1gb
--distribution
Long option   short option      description
--nodes=1      -N         request num of servers
--ntasks=1      -n         num tasks that job will use (useful for MPI applications)
--cpus-per-task=8   -c

If you invest in 10 cores, burst qos can use up to 90 cores!
#SBATCH --nodes=1

Task Arrays
#SBATCH --array=1-200%10   &lt;== run on 10 jobs at a time to be nice
$SLURM_ARRAY_TASK_ID
%A: job id
%a: task id

HPG COMMANDS:
id            &lt;== see your user id, your group id, etc.
sbatch   &lt;== submit script.sh to scheduler
sbatch --qos=phz5155-b   &lt;==
squeue         &lt;== see ALL jobs running
squeue -u rosedj1   &lt;== just see your jobs
squeue -j &lt;job_id&gt;
scancel &lt;job_id&gt;   &lt;== kill a job
sacct         &lt;==
sstat         &lt;==
slurmInfo         &lt;== see info about resource utilization; must do: module load ufrc
slurmInfo -p      &lt;== partition, a better summary
slurmInfo -g &lt;group_name&gt;   &lt;==
srun --mpi=pmix_v2 myApp

Memory utilization = MAX amount used at one point
Memory request = aim for 20-50% of total use

BE WISE ABOUT USING RESOURCES!
- Users have taken up 16 cores and TOOK MORE TIME than just using 1 core!!!

It would be interesting write a SLURM script which submits
many of the same job with different cores, plots the efficiency vs. num cores

QOS or burstQOS
"Quality of Service"
When you do sbatch, the -b option is “burst capacity” to allow 9x allocation of resources when resources are idle
--qos=phz5155-b
--qos=<group>

*In the job summary email, the memory usage is talking about RAM efficiency*

Time:
-t
time limit is 31 days
- It is to our benefit to be accurate with job time
- infinite loops will just waste resources and make you think your job is actually working
- the scheduler might postpone your job if it sees it will delay other people's jobs

Module system organizes file paths
If you want to use common modules on HPG, you must load them first:
module load <module>
module load python
module load python3
module load = ml    &lt;== already aliased automagically into HPG
module list      &lt;== list modules
module spider      &lt;== list everything?
module spider cl   &lt;== list everything with cl in name
module purge      &lt;== unloads all modules
ml intel         &lt;== allows you to do "make" commands
module load intel/2018 openmpi/3.1.0   &lt;== compiling

Learning about Xpra:
module load gui
launch_gui_session -h   &lt;== shows help options
- This will load a session on a graphical node on the cluster
- Default time on server is 4 hrs
- use the -a option to use secondary account
- use the -b option to use burst SLURM qos

Paste the xpra url into your local terminal

Do:
module load gui
launch_gui_session -e <executable>   (e.g., launch_rstudio_gui)
xpra attach ssh:<stuff>
xpra_list_sessions
scancel &lt;job_id&gt;

ln -s &lt;file_path_that's_way_far_away&gt; &lt;dest_path&gt;   &lt;== makes a symbolic link from &lt;far_file&gt; to &lt;dest_path&gt;

Development Sessions
When to use a dev session?
- When a job requires multiple cores and maybe a few days to run
- There are 6 dev nodes!

module load ufrc
srundev -h               &lt;== help!
srundev --time=04:00:00      &lt;== begin a 4 hr dev session, with the default 1 processor core and 2 GB of memory
srundev --time=60 --ntasks=1 --cpus-per-task=4 --mem=4gb      &lt;== additional flags
srundev -t 3-0             &lt;== session lasts 3 days
srundev -t 60               &lt;== session lasts 60 min
- default time is 00:10:00 (10 min) and max time is 12:00:00
These are all wrappers for:
srun --partition=hpg2-dev --pty bash -i

Getting CMSSW on HPG!!!
1. Start a dev session
2. source /cvmfs/cms.cern.ch/cmsset_default.sh    &lt;== this makes cmsrel and cmsenv two new aliases for you!
3. Now cmsrel your favorite CMSSW_X_Y_Z

Misc Info on HPG:
Terminology:
8 cpus/task = 8 cores on that one server
1 node has: RAM, maybe 2 sockets (processors), each with 2 cores
A node is a physical server
Each server has either 2 or 4 sockets
You can also specify the number of tasks per processor: ntasks-per-socket
each processor only has a certain bandwidth to memory
processor = cpu= core
15000 servers all networked together, each node has either 32 or 64 cores on it

Entire PHZ5155 course is allocated a whole node!
- This is 32 cores on HPG2
The slowdown of your job may be in the bandwidth!

Data center near Satchel’s where HiPerGator resides
51000 cores in HPG2 cluster
(only 14000 cores in HPG1)
world-class cluster
3 PB of storage

It’s essentially just a big rack of computers, where each computer has:
HPG2: 2 servers, 32 cores per server, 128 GB RAM/core?
HPG1: 1 servers, 64 cores per server, 256 GB RAM/core?

hpg1 is 3 years old and has older nodes
3.5 GB/core available

threaded=parallel=open MPI

Can do parallel applications:
- OpenMP, Threaded, Pthreads applications
- all cores on ONE server, shared memory
- CAN'T talk to other servers
MPI (Message Passing Interface)
- applications which can run across multiple servers

ntasks = # of MPI rinks
say you want to run 100 tasks across 10 nodes
100 MPI ranks
You might think the scheduler would put 10 MPI ranks on each node,
- but it won't be so equal per node, necessarily!
The scheduler may put 30 tasks on one node, and distribute the remaining 70 tasks on other nodes.
Though you can control the ntasks-per-node
Two processors, each processor has 2 cores
16 cores per processor
64 cores per node

For Windows users who need a Terminal:
- MobaXterm
- or the Ubuntu subsystem
Need an SFTP client to move from to your computer
- Cyberduck
- FileZilla
Text editor:
- BBedit(?)

Cluster basics:
ssh’ing puts you into a login node
Then submit a job to the scheduler.
- The scheduler submits the job to the 51000 cores!
- You must prepare a file to tell scheduler what to do (BATCH script)
    - number of CPUS
    - RAM
    - how long to process the job

There are also compute nodes
- this is where the money is!
- They are optimized to distribute jobs across different computers efficiently

*Mantra:* <br />
"GUIs make easy tasks easier; CLIs make difficult tasks possible."
